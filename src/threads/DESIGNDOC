            +--------------------+
            |        EE 415      |
            | PROJECT 1: THREADS |
            |   DESIGN DOCUMENT  |
            +--------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

JuHyeon Jang	 <cjdool@kaist.ac.kr>
DongEon Kim		 <dongeon97@kaist.ac.kr>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

	<pintos/src/threads/thread.h>

	struct thread 
	{
		...

		//Alarm Clock
		int64_t ticks_wakeup;				/* Ticks to wakeup thread */

		...
	}

	<pintos/src/threads/thread.c>

	static struct list sleep_list;			/* List of process in sleeping(BLOCKED) state */
	static int64_t next_tick_to_wakeup = INT64_MAX;  /* Minimum ticks in sleep_list */
											

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

	timer_sleep을 call한 thread가 thread_sleep()을 통해서 sleep_list에 들어가고
	wakeup해아할 tick을 ticks_wakeup에 저장, next_tick_to_wakeup과 비교해서 update한다.
	매 tick마다 timer interrupt가 발생하는데, handler가 next_tick_to_wakeup과 현재 tick을
	비교해서 thread_wakeup() 함수 호출, wakeup할 thread를 unblock하고 sleeplist에서 제거,
	next_tick_to_wakeup을 update한다.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?
	
	매 tick마다 thread_wakeup() 함수를 호출하지 않고, next_tick_to_wakeup 변수를 사용해서
	필요할 때만 함수 호출 

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

	thread가 sleeplist에 push될 때 interrupt를 disable한다.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

	thread가 sleeplist에 push될 때 interrupt를 disable한다.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

	busy waiting을 피하기 위해 sleep, wake를 사용해야 했고	
	struct list, intr_disable(), intr_set_level()가 구현되어있었기 때문에
	sleep_list를 만들어	list에 관련한 함수들을 사용하고 
	race condition을 피하기 위해 interrupt를 조절할 수 있었다.

             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

	<pintos/src/threads/thread.h>

	struct thread 
	{
		...

		//Priority scheduler
		int old_priority;					/* The initial priority */
		struct lock *wait_on_lock;			/* The lock thread waits */
		struct list donations;				/* donation list */
		struct list_elem d_elem;			/* donation list element */

		...
	}
>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

	lock, semphore, condition variable 모두 wait할 때, 
	list_insert_ordered() 함수를 사용하여 priority 순서에 맞게 list에 넣고
	wakeup할 때, priority donation을 고려하여 list_sort해주고 list의 가장 앞에 있는 
	element를 pop하여 unblock해주었다.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

	thread가 lock_acquire할 때 request한 lock의 holer가 있을 경우, lock holder thread에게
	priority donation을 하고 lock holder의 donations list에 insert해준다.
	그리고 해당 lock의 wait list에 priority 순서대로 insert하고 thread_block()된다. 

	Nested donation은 Thread H가 request한 lock을 Thread M이 hold, 
	Thread M이 request한 lock을 Thread L이 hold한 경우이다.
	그래서 lock acquire을 시도할 때 lock A holder가 기다리고 있는 lock B가 있는지 확인하고
	lock B holder에도 priority donation을 진행한다. 이때 Thread M은 Thread H에게 donation 받은
	priority를 Thread L에게 donate하는 경우이므로, Thread L의 donation list에 
	Thread H는 insert하지 않는다.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

	Thread H가 wait중인 lock A가 해제되는 경우, lock A holder의 donation list 중에서 
	lock A를 기다리고 있던 thread들을 list에서 제거하고 lock A holder의 priority는 donation list
	에 남아있는 thread들의 priority와 자신의 초기 priority중 가장 높은 것으로 재설정한다.
	
	그 이후 lock A의 wait list를 priority 순서대로 sort하고 가장 높은 thread를 unblock한다

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

	Nested donation에서 thread H의 priority를 donate받은 모든 thread의 donation list에 thread H를
	insert하지 않아서 lock acquire, release단계에서 더 간단해졌다.

	current_thread는 가장 높은 priority를 가진 thread임을 이용하여 필요없는 조건문들을 사용하지 않았다.


              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

    <pintos/src/thread/thread.h>

    struct thread
    {
        ...

        //ADVANCED SCHEDULER
        int nice;                 /* Nice value */
        int recent_cpu;           /* Recent cpu counter */
    }

    <pintos/src/threads/thread.c>

    static int load_avg;          /* Load average value for MLFQ scheduler */

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59     A
 4      4   0   0  62  61  59     A
 8      8   0   0  61  61  59     A
12     12   0   0  60  61  59     B
16     12   4   0  60  60  59     B
20     12   8   0  60  59  59     A
24     16   8   0  59  59  59     A
28     20   8   0  58  59  59     C
32     20   8   4  58  59  58     B
36     20  12   4  58  58  58     B

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

    먼저, load_avg, recent_cpu, priority 재계산 부분에서 순서가 정해져 있지 않은
    부분 때문에 값들에 차이를 보일 수 있을 것 같다.
    특히 1초가 100개의 tick으로 구성되는데, 매 100번째 tick마다 순서가 문제가 된다.
    100번째 tick에서 계산하는 부분은 recent cpu를 1증가 시키는 부분(inc_recent_cpu()),
    load_avg를 계산하는 부분(calculate_load_avg()), recent_cpu를 재계산하는 부분
    (recalculate_recent_cpu), priority를 재계산하는 부분(recalculate_priority)이다.
    해당 부분의 순서가 정확하게 명시되어 있지 않아서 아래와 같은 순서로 진행한다.
    1. inc_recent_cpu(): 매 tick마다
    2. calculate_load_avg(): 매 100 tick마다
    3. recalculate_recent_cpu(): 매 100 tick마다
    4. recalculate_priority(): 매 4 tick마다
    해당 순서로 진행할 때 모든 테스트를 통과 할 수 있었다.
    두번째로 인터럽트 핸들러에 인터럽트를 끄는 부분이다.
    기존 소스코드에 비해서 인터럽트 핸들러 코드가 길어져서 어떤 경우는 인터럽트 핸들러
    코드를 처리하는 중에 다시 인터럽트 핸들러가 호출되는 문제가 발생했다.
    해당 문제를 해결하기 위해서는 보통 인터럽트 pending queue를 두는 경우가 일반적인데,
    핀토스에서는 전체구현이 힘들어 추가적인 코드를 진행할 때는 인터럽트를 끄는 방향으로 진행했다.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

   인터럽트가 꺼진상태로 진행되는 부분은 thread의 상태가 변경되어 sleep_list나 ready_list로
   들어가는 부분과 타이머 인터럽트가 수행되는 부분밖에 없다.
   thread의 상태가 변경되어 특정한 list로 삽입되는 경우는 코드라인이 짧아서 큰 영향을 주지
   않을 것으로 예상되고 기존의 핀토스 코드와 큰 차이가 없다.
   타이머 인터럽트부분이 상대적으로 길어졌고, 특히 thread latency를 보기 위해서 thread마다 
   tick을 카운트 하는 부분이 추가되었을 때, 인터럽트가 꺼지지 않으면 load_avg 값에서 문제가 
   발생했다.
   성능과 정확성을 최대한 맞추기 위해서 mlfq와 thread latency 코드가 추가되는 부분에 한해서
   인터럽트를 끄는 것으로 타협을 맞추었다.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

    Advantage:
    1. Modulized Design
        - 되도록 특정 기능이 재사용 될 수 있게 모듈화 하여 작성 
    2. Default value Macro
        - 전처리기를 사용해서 초기화 값을 세팅할 수 있게 하여, 수정을 쉽게 하도록 구성
    Disadvantage:
    1. Readability
        - Fixed point 계산등에서 계산식을 1줄로 작성하여, 가독성이 부족
    2. Race condition in multi-core system
        - load_avg를 여러코어에서 동시에 계산 및 갱신을 시도하면 race condition이 발생 가능
    Future Improvement:
        - Multiple Ready-queue: ready queue가 priority마다 존재하는 MLFQ를 구성
        - Race condition fix: race condition이 발생하는 부분에 synchronization primitives나
                              atomic 함수를 도입하여 수정

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

   계산부분이 여러과정에서 사용되기 때문에, 다른 파일로 제작했다.
   fixed point에 대해 다른 구조체나 type을 만들기 보다는 int형으로 해석만 다르게 하는 전략을
   선택했다.
   이를 위해 실제 int형이 정보를 fixed point integer형으로 양뱡향으로 전환하는 함수, 전환시
   반올림을 하는 전환 함수를 추가했다.
   priority와 nice는 실제 integer형을 취하고, recent_cpu와 load_avg는 fixed point integer형을
   취해서 이를 해석하기 위한 함수의 추가와 같다.
   기본적으로 fixed point integer형에 대한 사칙연산과 fixed point integer와 실제 integer형에 
   대한 사칙연산도 추가했다.
   MLFQ의 연산식을 계산하기 위해서 2가지 형태의 사칙연산을 추가한 것과 같다.

               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
